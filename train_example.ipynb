{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from balm.config import BalmConfig, BalmMoEConfig#, BalmExpertChoiceMoEConfig\n",
    "from balm.data import load_dataset, DataCollator\n",
    "from balm.models import (\n",
    "    BalmForMaskedLM,\n",
    "    BalmModel,\n",
    "    BalmMoEForMaskedLM,\n",
    "    #BalmExpertChoiceMoEForMaskedLM,\n",
    "    #BalmHybridMoEForMaskedLM,\n",
    "    #BalmMoERoPEForMaskedLM,\n",
    ")\n",
    "from balm.config import BalmConfig\n",
    "from balm.tokenizer import Tokenizer\n",
    "from balm.train import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab=\"./vocab.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sep(txt):\n",
    "    return txt.replace(\"</s>\", \"<cls><cls>\")\n",
    "\n",
    "\n",
    "data_files = {\n",
    "    \"train\": \"./balm/test_data/test_1k.txt\",\n",
    "    \"test\": \"./balm/test_data/test_1k.txt\",\n",
    "    \"eval\": \"./balm/test_data/test_1k.txt\",\n",
    "}\n",
    "\n",
    "# data_files = {\n",
    "#     \"train\": \"../train-test-eval_paired/train.txt\",\n",
    "#     \"test\": \"../train-test-eval_paired/test.txt\",\n",
    "#     \"eval\": \"../train-test-eval_paired/eval.txt\",\n",
    "# }\n",
    "\n",
    "# data_files = {\n",
    "#     \"train\": \"../jaffe-plusHD_clust0.9_split/train.txt\",\n",
    "#     \"test\": \"../jaffe-plusHD_clust0.9_split/test.txt\",\n",
    "#     \"eval\": \"../jaffe-plusHD_clust0.9_split/eval.txt\",\n",
    "# }\n",
    "\n",
    "dataset = load_dataset(\"text\", data_files=data_files, preprocess_fn=remove_sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf02e0924e94947871e8addbba516b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22041d4befae4748b3287e77b0136954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fdf6918289d49dd8e1c5626a6db5e6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    lambda x: tokenizer(\n",
    "        x[\"text\"],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=320,\n",
    "    ),\n",
    "    remove_columns=\"text\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollator(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # matched to ESM-2 8M\n",
    "config = BalmConfig(\n",
    "    embed_dim=320,\n",
    "    ffn_dim=320 * 4,\n",
    "    num_layers=6,\n",
    "    num_heads=20,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    ")\n",
    "model = BalmForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6294113"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"eval\"],\n",
    "    output_dir=\"./training_runs/save_tests\",\n",
    "    epochs=1,\n",
    "    logging_steps=5,\n",
    "    eval_steps=100,\n",
    "    warmup_steps=10,\n",
    "    # save_steps=15,\n",
    "    per_device_train_batch_size=32,\n",
    "    # use_cpu=True,\n",
    "    # use_wandb=True,\n",
    "    # wandb_project=\"test_wandb_logging\",\n",
    "    # wandb_entity=\"bryanbriney\",\n",
    "    run_name=\"save_test_001\",\n",
    "    deepspeed=True,\n",
    "    deepspeed_config='deepspeed_config_stage1.json'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-28 15:44:46,084] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible\n",
      "[2024-05-28 15:44:47,188] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.2, git-hash=unknown, git-branch=unknown\n",
      "[2024-05-28 15:44:47,189] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-05-28 15:44:47,189] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mpi4py'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/shared/Sarah/current/pytorch-training/balm/train/trainer.py:245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    242\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mdefine_metric(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglobal_step\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    243\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mdefine_metric(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m, step_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglobal_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, step_sync\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler \u001b[38;5;241m=\u001b[39m get_scheduler(\n\u001b[1;32m    249\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer,\n\u001b[1;32m    250\u001b[0m     num_warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_warmup_steps,\n\u001b[1;32m    251\u001b[0m     num_training_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train_steps,\n\u001b[1;32m    252\u001b[0m )\n",
      "File \u001b[0;32m~/shared/Sarah/current/pytorch-training/balm/train/trainer.py:482\u001b[0m, in \u001b[0;36mTrainer.wrap_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdeepspeed\u001b[39;00m\n\u001b[1;32m    481\u001b[0m     model_params \u001b[38;5;241m=\u001b[39m [p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters() \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mrequires_grad]\n\u001b[0;32m--> 482\u001b[0m     model, optimizer, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mdeepspeed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath_to_deepspeed_config.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/deepspeed/__init__.py:143\u001b[0m, in \u001b[0;36minitialize\u001b[0;34m(args, model, optimizer, model_parameters, training_data, lr_scheduler, distributed_port, mpu, dist_init_required, collate_fn, config, config_params)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepspeed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m comm \u001b[38;5;28;01mas\u001b[39;00m dist\n\u001b[1;32m    142\u001b[0m dist_backend \u001b[38;5;241m=\u001b[39m get_accelerator()\u001b[38;5;241m.\u001b[39mcommunication_backend_name()\n\u001b[0;32m--> 143\u001b[0m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_distributed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdist_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdist_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mdistributed_port\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistributed_port\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mdist_init_required\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdist_init_required\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# Set config using config_params for backwards compat\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m config_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/deepspeed/comm/comm.py:658\u001b[0m, in \u001b[0;36minit_distributed\u001b[0;34m(dist_backend, auto_mpi_discovery, distributed_port, verbose, timeout, init_method, dist_init_required, config, rank, world_size)\u001b[0m\n\u001b[1;32m    656\u001b[0m         patch_aws_sm_env_for_torch_nccl_backend(verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 658\u001b[0m         \u001b[43mmpi_discovery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistributed_port\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistributed_port\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cdb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m cdb\u001b[38;5;241m.\u001b[39mis_initialized():\n\u001b[1;32m    661\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mint\u001b[39m(os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRANK\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/deepspeed/comm/comm.py:677\u001b[0m, in \u001b[0;36mmpi_discovery\u001b[0;34m(distributed_port, verbose)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmpi_discovery\u001b[39m(distributed_port\u001b[38;5;241m=\u001b[39mTORCH_DISTRIBUTED_DEFAULT_PORT, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    674\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;124;03m    Discovery MPI environment via mpi4py and map to relevant dist state\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmpi4py\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MPI\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msubprocess\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     comm \u001b[38;5;241m=\u001b[39m MPI\u001b[38;5;241m.\u001b[39mCOMM_WORLD\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mpi4py'"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
